{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47d57ba-1e56-451f-b0b8-0adaf2a29174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I can't believe you did such a \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "generator = pipeline(\"text-generation\", tokenizer=AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\", add_bos_token=True),\n",
    "                     model=\"apple/OpenELM-270M-Instruct\", trust_remote_code=True)\n",
    "\n",
    "#generator(\"I can't believe you did such a \", do_sample=False)\n",
    "\n",
    "# -> Somehow this pipeline doesn't work properly for apple models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835bb762-28cf-4d22-860b-04c19c26f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From transformers-from-scratch tutorial\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "\n",
    "class TransformerSampler:\n",
    "    \"\"\"Class to sample from \n",
    "    \n",
    "    Adapted from the excellent transformer-from-scratch course:\n",
    "    https://arena3-chapter1-transformer-interp.streamlit.app/%5B1.1%5D_Transformer_from_Scratch\n",
    "    https://github.com/callummcdougall/ARENA_3.0\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "        '''\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how \n",
    "        new tokens are chosen.\n",
    "        '''\n",
    "        self.model.eval()\n",
    "        tokens = torch.tensor([self.tokenizer.encode(prompt)])\n",
    "\n",
    "        for i in range(max_tokens_generated):\n",
    "            logits = self.model(tokens).logits[0,-1]\n",
    "            next_token = self.sample_next_token(tokens[0], logits, **kwargs)\n",
    "            \n",
    "            # Append the generated token\n",
    "            tokens = torch.cat((tokens, torch.tensor([[next_token]])), 1)\n",
    "\n",
    "            # Break if EOS token generated\n",
    "            if next_token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return self.tokenizer.decode(tokens[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"], \n",
    "        logits: Float[Tensor, \"seq_len d_vocab\"], \n",
    "        temperature=1.0, \n",
    "        top_k=0, \n",
    "        top_p=0.0, \n",
    "        frequency_penalty=0.0,\n",
    "        seed=None\n",
    "    ):\n",
    "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "        assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Apply all the specialized sampling methods\n",
    "        if temperature == 0:\n",
    "            return TransformerSampler.greedy_search(logits)\n",
    "        elif temperature != 1.0:\n",
    "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
    "        if frequency_penalty != 0.0:\n",
    "            logits = TransformerSampler.apply_frequency_penalty(input_ids, logits, frequency_penalty)\n",
    "        if top_k > 0:\n",
    "            return TransformerSampler.sample_top_k(logits, top_k)\n",
    "        if top_p > 0.0:\n",
    "            return TransformerSampler.sample_top_p(logits, top_p)\n",
    "        return TransformerSampler.sample_basic(logits)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(\n",
    "        logits: Float[Tensor, \"d_vocab\"]\n",
    "    ) -> int:\n",
    "        '''\n",
    "        Returns the most likely token (as an int).\n",
    "        '''\n",
    "        out = logits.argmax().item()\n",
    "        return out\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_temperature(\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        temperature: float\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Applies temperature scaling to the logits.\n",
    "        '''\n",
    "        return logits / temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_frequency_penalty(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        freq_penalty: float\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Applies a frequency penalty to the logits.\n",
    "        '''\n",
    "        freqs = torch.bincount(input_ids)\n",
    "        if len(freqs)<len(logits):\n",
    "            freqs = t.cat([freqs, torch.zeros(len(logits)-len(freqs))], dim=0)\n",
    "        return logits - freq_penalty*freqs\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_basic(\n",
    "        logits: Float[Tensor, \"d_vocab\"]\n",
    "    ) -> int:\n",
    "        '''\n",
    "        Samples from the distribution defined by the logits.\n",
    "        '''\n",
    "        m = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        return m.sample()\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_k(\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        k: int\n",
    "    ) -> int:\n",
    "        '''\n",
    "        Samples from the top k most likely tokens.\n",
    "        '''\n",
    "        top_k = torch.topk(logits, k)\n",
    "        top_logits, top_indices = top_k.values, top_k.indices\n",
    "        m = torch.distributions.categorical.Categorical(logits=top_logits)\n",
    "        index = m.sample()\n",
    "        return top_indices[index]\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        top_p: float,\n",
    "        min_tokens_to_keep: int = 1\n",
    "    ) -> int:\n",
    "        '''\n",
    "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
    "        '''\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        \n",
    "        # Sort the probabilities from largest to smallest\n",
    "        sorted_probs = torch.sort(probs, descending=True).values  # Also has .indices\n",
    "\n",
    "        # Find the cutoff point where the cumulative probability first equals or exceeds top_p.\n",
    "        # We do the cutoff inclusively, keeping the first probability above the threshold.\n",
    "        cutoff_index = torch.where(torch.cumsum(sorted_probs, dim=0) >= top_p)[0][0]\n",
    "        cutoff_prob = sorted_probs[max(cutoff_index, min_tokens_to_keep)]\n",
    "        # FIXME: If cutoff_prob is exactly matched for several indices, we might get too many values\n",
    "\n",
    "        # If the number of kept probabilities is less than min_tokens_to_keep, keep that many tokens instead.\n",
    "\n",
    "        # Set all other probabilities to zero\n",
    "        masked_probs = probs.masked_fill(probs<cutoff_prob, 0)\n",
    "        masked_probs /= masked_probs.sum()\n",
    "        \n",
    "        # Normalize and sample\n",
    "        m = torch.distributions.categorical.Categorical(probs=masked_probs)\n",
    "        return m.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b254c8b3-de86-4e92-be4a-ce90208f2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "\n",
    "model = Transformer('apple/OpenELM-270M', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "361a3c11-8c81-4f30-af5e-974f5784cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TransformerSampler(model=model.model, tokenizer=model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b6e3668-bb60-4a65-a8d4-d5ad527f5140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt: 'Jingle bells, jingle bells, jingle all the way'\n",
      "\n",
      "Your model said: \"<s> Jingle bells, jingle bells, jingle all the way!\\nThe holiday season is upon us and it's time to get your holiday shopping done.\\nIf you're like me, you're probably looking for a new pair of shoes to wear to work or to the gym.\\nI'm not a big fan of shoes\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=64, temperature=0.0)\n",
    "print(f\"Your model said: {output!r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31ee4e-92ad-4991-9709-b71944f52163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
