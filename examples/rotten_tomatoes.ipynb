{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7eb9fc-c5bf-436f-ad25-746ba8072741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4fc2fe-1d50-4003-8dbd-ff0859de7c6e",
   "metadata": {},
   "source": [
    "# Example: Rotten Tomatoes\n",
    "\n",
    "In this example we compare two models on 0-shot sentiment detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffcf2c-68be-4fce-bc22-ad3811b2bccb",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e77af4-47fb-49a6-a429-43f15f34044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perspectival.model import Transformer\n",
    "\n",
    "model = Transformer('gpt2')\n",
    "model2 = Transformer('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2192c4-ab87-4b37-81be-a26bade2e22c",
   "metadata": {},
   "source": [
    "## Set up the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e80a88-f556-44e7-ad3c-870b169dd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perspectival.loader import load_rotten_tomatoes\n",
    "from perspectival.experiment import Experiment\n",
    "\n",
    "dataset, features = load_rotten_tomatoes()\n",
    "experiment = Experiment(dataset=dataset, name='Rotten Tomato Test', features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2db55c8-7edd-4e43-ba21-4a9fb3d81b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiment.sample(num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9aa66-347c-40df-9e5b-fe87e9e8bea4",
   "metadata": {},
   "source": [
    "## Compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82a9e00-dd46-49a3-9371-7aef7099f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.compute_correctness(models=[model, model2])\n",
    "experiment.compute_disagreement(models=[model, model2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0ee14-5fe4-4559-bf44-4274274d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment.name = 'test'\n",
    "#experiment.save('../data/')\n",
    "#experiment = Experiment.load('../data/', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f958b3-252c-4c9a-9850-2b269608d189",
   "metadata": {},
   "source": [
    "## Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032e4dbc-5453-473e-a005-f82c30ba6674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM (train_129)\n",
      "\"\"\"Question: What is the sentiment of the text below?\n",
      "\n",
      "Text: 'it is nature against progress . in fessenden's horror trilogy , this theme has proved important to him and is especially so in the finale .'\n",
      "\n",
      "Answer: The sentiment is\"\"\"\n",
      "Options: ['negative', 'positive']\n",
      "\n",
      "FEATURES\n",
      "GroundTruth 1\n",
      "OptionLogLikelihood gpt2 [-7.09939  -6.559614]\n",
      "OptionLogLikelihood distilgpt2 [-6.6436863 -6.8264008]\n",
      "ModelChoices gpt2 1\n",
      "ModelChoices distilgpt2 0\n",
      "PredictionCorrectness gpt2 1.0\n",
      "PredictionCorrectness distilgpt2 0.0\n",
      "LogDisagreement ('distilgpt2', 'gpt2') 0.098624855\n",
      "BinaryDisagreement ('distilgpt2', 'gpt2') True\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "ITEM (train_4523)\n",
      "\"\"\"Question: What is the sentiment of the text below?\n",
      "\n",
      "Text: 'to build a feel-good fantasy around a vain dictator-madman is off-putting , to say the least , not to mention inappropriate and wildly undeserved .'\n",
      "\n",
      "Answer: The sentiment is\"\"\"\n",
      "Options: ['negative', 'positive']\n",
      "\n",
      "FEATURES\n",
      "GroundTruth 0\n",
      "OptionLogLikelihood gpt2 [-6.2675176 -6.489197 ]\n",
      "OptionLogLikelihood distilgpt2 [-6.7789025 -6.521552 ]\n",
      "ModelChoices gpt2 0\n",
      "ModelChoices distilgpt2 1\n",
      "PredictionCorrectness gpt2 1.0\n",
      "PredictionCorrectness distilgpt2 0.0\n",
      "LogDisagreement ('distilgpt2', 'gpt2') 0.057049245\n",
      "BinaryDisagreement ('distilgpt2', 'gpt2') True\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show items with max disagreement\n",
    "scores = experiment.get_feature('LogDisagreement', models=(model.name, model2.name)).values\n",
    "samples = experiment.sample(num=2, sampling_method='last', ordering_scores=scores)\n",
    "samples.display_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb65a8a-de28-45be-81bf-9af2da77d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "- Accuracy: 0.61\n",
      "- Output: [(1, 91), (0, 9)]\n",
      "distilgpt2\n",
      "- Accuracy: 0.58\n",
      "- Output: [(1, 88), (0, 12)]\n"
     ]
    }
   ],
   "source": [
    "# General output statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "for m in [model, model2]:\n",
    "    print(m.name)\n",
    "    accuracy = np.mean(experiment.get_feature('PredictionCorrectness', model=m.name).values)\n",
    "    print(f\"- Accuracy: {accuracy:.2f}\")\n",
    "    print(\"- Output:\", Counter(experiment.get_feature('ModelChoices', model=m.name).values).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ff9c47-dff0-4588-9a72-1a8a0d741fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall disagreement rate: 0.07\n",
      "Accuracy of gpt2: 0.61\n",
      "Accuracy of distilgpt2: 0.58\n"
     ]
    }
   ],
   "source": [
    "scores = experiment.get_feature('LogDisagreement', models=(model.name, model2.name)).values\n",
    "disagreement_rate = sum(scores>0)/len(scores)\n",
    "print(f\"Overall disagreement rate: {disagreement_rate:.2f}\")\n",
    "\n",
    "for m in [model, model2]:\n",
    "    accuracy = np.mean(experiment.get_feature('PredictionCorrectness', model=m.name).values)\n",
    "    print(f\"Accuracy of {m.name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358de0c-01d1-4505-85bb-b11249a6da83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
