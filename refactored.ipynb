{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7eb9fc-c5bf-436f-ad25-746ba8072741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253a8e5d-9034-47b2-b53c-4efae2e2f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import load_anthropic_eval_dataset\n",
    "\n",
    "experiment = load_anthropic_eval_dataset('/Users/john/code/anthropic_evals', 'advanced-ai-risk', 'human_generated_evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e80a88-f556-44e7-ad3c-870b169dd01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rotten_tomatoes couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/john/.cache/huggingface/datasets/rotten_tomatoes/default/0.0.0/aa13bc287fa6fcab6daf52f0dfb9994269ffea28 (last modified on Fri May 10 14:54:20 2024).\n"
     ]
    }
   ],
   "source": [
    "from loader import load_rotten_tomatoes\n",
    "from experiment import Experiment\n",
    "\n",
    "dataset = load_rotten_tomatoes()\n",
    "experiment = Experiment(dataset=dataset, name='Rotten Tomato Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e77af4-47fb-49a6-a429-43f15f34044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer, LazyTransformer\n",
    "\n",
    "model = LazyTransformer('gpt2')\n",
    "#model2 = LazyTransformer('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2db55c8-7edd-4e43-ba21-4a9fb3d81b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiment.sample(num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042270eb-cee2-4089-9581-e1c88631d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import *\n",
    "\n",
    "for m in [model, model2]:\n",
    "    lls = OptionLogLikelihood.compute(experiment.dataset, model=m)\n",
    "    experiment.register_feature(lls)\n",
    "\n",
    "    model_choices = ModelChoices.compute(lls)\n",
    "    experiment.register_feature(model_choices)\n",
    "    #experiment.register_feature(ModelChoices.compute(experiment.get_feature('OptionLogLikelihood', m.name)))\n",
    "\n",
    "    experiment.register_feature(PredictionCorrectness.compute(experiment.dataset, model_choices))\n",
    "\n",
    "log_disagreement = LogDisagreement.compute( (experiment.get_feature('OptionLogLikelihood', model.name),\n",
    "                                            experiment.get_feature('OptionLogLikelihood', model2.name)) )\n",
    "experiment.register_feature(log_disagreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62d0ee14-5fe4-4559-bf44-4274274d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment.name = 'test'\n",
    "#experiment.save('data/')\n",
    "experiment = Experiment.load('data/', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "032e4dbc-5453-473e-a005-f82c30ba6674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM (train_7865)\n",
      "\"\"\"Question: What is the sentiment of the text below?\n",
      "\n",
      "Text: 'it has more in common with a fireworks display than a movie , which normally is expected to have characters and a storyline .'\n",
      "\n",
      "Answer: The sentiment is\"\"\"\n",
      "Options: ['negative', 'positive']\n",
      "Ground truth: negative\n",
      "\n",
      "FEATURES\n",
      "OptionLogLikelihood gpt2 -6.6571083\n",
      "ModelChoices gpt2 1\n",
      "PredictionCorrectness gpt2 0.0\n",
      "LogDisagreement ('gpt2', 'gpt2') -0.0\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "ITEM (train_5439)\n",
      "\"\"\"Question: What is the sentiment of the text below?\n",
      "\n",
      "Text: 'the whole film has this sneaky feel to it Â– as if the director is trying to dupe the viewer into taking it all as very important simply because the movie is ugly to look at and not a hollywood product .'\n",
      "\n",
      "Answer: The sentiment is\"\"\"\n",
      "Options: ['negative', 'positive']\n",
      "Ground truth: negative\n",
      "\n",
      "FEATURES\n",
      "OptionLogLikelihood gpt2 -6.536299\n",
      "ModelChoices gpt2 1\n",
      "PredictionCorrectness gpt2 0.0\n",
      "LogDisagreement ('gpt2', 'gpt2') -0.0\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = experiment.get_feature('LogDisagreement', models=(model.name, model2.name)).values\n",
    "samples = experiment.sample(num=2, sampling_method='last', ordering_scores=scores)\n",
    "samples.display_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddb65a8a-de28-45be-81bf-9af2da77d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "- Accuracy: 0.54\n",
      "- Output: [(1, 174), (0, 26)]\n",
      "distilgpt2\n",
      "- Accuracy: 0.59\n",
      "- Output: [(1, 169), (0, 31)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for m in [model, model2]:\n",
    "    print(m.name)\n",
    "    accuracy = np.mean(experiment.get_feature('PredictionCorrectness', model=m.name).values)\n",
    "    print(f\"- Accuracy: {accuracy:.2f}\")\n",
    "    print(\"- Output:\", Counter(experiment.get_feature('ModelChoices', model=m.name).values).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ff9c47-dff0-4588-9a72-1a8a0d741fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall disagreement rate: 0.24\n",
      "Unproblematic behavior of gpt2: 0.34\n",
      "Unproblematic behavior of distilgpt2: 0.18\n",
      "\n",
      "Category 'coordinate-other-ais':\n",
      "- Disagreement rate: 0.24\n",
      "- Scores: gpt2 0.34, distilgpt2 0.18\n"
     ]
    }
   ],
   "source": [
    "# For this dataset we want to see how disagreement varies for categories\n",
    "scores = experiment.get_feature('LogDisagreement', models=(model.name, model2.name)).values\n",
    "disagreement_rate = sum(scores>0)/len(scores)\n",
    "print(f\"Overall disagreement rate: {disagreement_rate:.2f}\")\n",
    "\n",
    "for m in [model, model2]:\n",
    "    accuracy = np.mean(experiment.get_feature('PredictionCorrectness', model=m.name).values)\n",
    "    print(f\"Unproblematic behavior of {m.name}: {accuracy:.2f}\")\n",
    "\n",
    "categories = experiment.get_feature('Category').values\n",
    "for category in sorted(list(set(categories))):\n",
    "    category_mask = categories==category\n",
    "    category_selection = experiment.sample(mask=category_mask)\n",
    "    \n",
    "    category_disagreement = np.array(category_selection.get_feature('LogDisagreement', models=(model.name, model2.name)).values)\n",
    "    accuracy1 = np.mean(category_selection.get_feature('PredictionCorrectness', model=model.name).values)\n",
    "    accuracy2 = np.mean(category_selection.get_feature('PredictionCorrectness', model=model2.name).values)\n",
    "    print(f\"\\nCategory '{category}':\")\n",
    "    print(f\"- Disagreement rate: {sum(category_disagreement>0)/len(category_disagreement):.2f}\")\n",
    "    print(f\"- Scores: {model.name} {accuracy1:.2f}, {model2.name} {accuracy2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb75ce-ca4f-4629-8a07-7675fd7c92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Adjust saving and loading? Or keep that on Experiment level? (Check risk that classes change and we loose the whole array)\n",
    "\n",
    "#TODO How to properly use dataset and experiment names? If it's not useful drop it\n",
    "\n",
    "#TODO try with better models (also make sure that this doesn't need tons of rewriting; consider model interface class)\n",
    "# - llama 8b vs llama 8b-instruct? (one such model requires 20GB of VRAM though!) -> make it possible to not keep all the models in memory!\n",
    "# - models from apple?\n",
    "\n",
    "#TODO resolve remaining todos in code\n",
    "# Add the test from the other notebook\n",
    "# Also, proper typing would be cool, but how to avoid the circular imports? Interface classes?\n",
    "\n",
    "#TODO upload to github and document properly\n",
    "\n",
    "# This finishes the initial scope and should be finished before London!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
