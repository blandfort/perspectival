{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7eb9fc-c5bf-436f-ad25-746ba8072741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253a8e5d-9034-47b2-b53c-4efae2e2f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import load_anthropic_eval_dataset\n",
    "\n",
    "experiment = load_anthropic_eval_dataset('/Users/john/code/anthropic_evals', 'advanced-ai-risk', 'human_generated_evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23e80a88-f556-44e7-ad3c-870b169dd01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rotten_tomatoes couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/john/.cache/huggingface/datasets/rotten_tomatoes/default/0.0.0/aa13bc287fa6fcab6daf52f0dfb9994269ffea28 (last modified on Fri May 10 14:54:20 2024).\n"
     ]
    }
   ],
   "source": [
    "from loader import load_rotten_tomatoes\n",
    "from experiment import Experiment\n",
    "\n",
    "dataset = load_rotten_tomatoes()\n",
    "experiment = Experiment(dataset=dataset, name='Rotten Tomato Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e77af4-47fb-49a6-a429-43f15f34044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LLMModel\n",
    "\n",
    "model = LLMModel('gpt2')\n",
    "model2 = LLMModel('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2db55c8-7edd-4e43-ba21-4a9fb3d81b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiment.sample(num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042270eb-cee2-4089-9581-e1c88631d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import *\n",
    "\n",
    "for m in [model, model2]:\n",
    "    lls = OptionLogLikelihood.compute(experiment.dataset, model=m)\n",
    "    experiment.register_feature(lls)\n",
    "\n",
    "    model_choices = ModelChoices.compute(lls)\n",
    "    experiment.register_feature(model_choices)\n",
    "    #experiment.register_feature(ModelChoices.compute(experiment.get_feature('OptionLogLikelihood', m.name)))\n",
    "\n",
    "    experiment.register_feature(PredictionCorrectness.compute(experiment.dataset, model_choices))\n",
    "\n",
    "log_disagreement = LogDisagreement.compute( (experiment.get_feature('OptionLogLikelihood', model.name),\n",
    "                                            experiment.get_feature('OptionLogLikelihood', model2.name)) )\n",
    "experiment.register_feature(log_disagreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62d0ee14-5fe4-4559-bf44-4274274d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment.name = 'test'\n",
    "#experiment.save('data/')\n",
    "experiment = Experiment.load('data/', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "032e4dbc-5453-473e-a005-f82c30ba6674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM (train_8080)\n",
      "\"\"\"Question: What is the sentiment of the text below?\n",
      "\n",
      "Text: 'the biggest problem with satin rouge is lilia herself . she's a cipher , played by an actress who smiles and frowns but doesn't reveal an inner life .'\n",
      "\n",
      "Answer: The sentiment is\"\"\"\n",
      "Options: ['negative', 'positive']\n",
      "Ground truth: negative\n",
      "\n",
      "FEATURES\n",
      "Category coordinate-other-ais\n",
      "OptionLogLikelihood gpt2 -6.5130873\n",
      "OptionLogLikelihood distilgpt2 -6.673251\n",
      "ModelChoices gpt2 1\n",
      "ModelChoices distilgpt2 0\n",
      "PredictionCorrectness gpt2 0.0\n",
      "PredictionCorrectness distilgpt2 1.0\n",
      "LogDisagreement ('distilgpt2', 'gpt2') 0.15345861\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "ITEM (train_7911)\n",
      "\"\"\"Question: What is the sentiment of the text below?\n",
      "\n",
      "Text: 'trapped presents a frightening and compelling 'what if ? ' scenario that will give most parents pause . . . . then , something terrible happens .'\n",
      "\n",
      "Answer: The sentiment is\"\"\"\n",
      "Options: ['negative', 'positive']\n",
      "Ground truth: negative\n",
      "\n",
      "FEATURES\n",
      "Category coordinate-other-ais\n",
      "OptionLogLikelihood gpt2 -6.2544947\n",
      "OptionLogLikelihood distilgpt2 -7.041103\n",
      "ModelChoices gpt2 1\n",
      "ModelChoices distilgpt2 0\n",
      "PredictionCorrectness gpt2 0.0\n",
      "PredictionCorrectness distilgpt2 1.0\n",
      "LogDisagreement ('distilgpt2', 'gpt2') 0.11217081\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO How to properly use dataset and experiment names? If it's not useful drop it\n",
    "\n",
    "scores = experiment.get_feature('LogDisagreement', models=(model.name, model2.name)).values\n",
    "samples = experiment.sample(num=2, sampling_method='last', ordering_scores=scores)\n",
    "samples.display_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddb65a8a-de28-45be-81bf-9af2da77d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "- Accuracy: 0.54\n",
      "- Output: [(1, 174), (0, 26)]\n",
      "distilgpt2\n",
      "- Accuracy: 0.59\n",
      "- Output: [(1, 169), (0, 31)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for m in [model, model2]:\n",
    "    print(m.name)\n",
    "    accuracy = np.mean(experiment.get_feature('PredictionCorrectness', model=m.name).values)\n",
    "    print(f\"- Accuracy: {accuracy:.2f}\")\n",
    "    print(\"- Output:\", Counter(experiment.get_feature('ModelChoices', model=m.name).values).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ff9c47-dff0-4588-9a72-1a8a0d741fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall disagreement rate: 0.24\n",
      "Unproblematic behavior of gpt2: 0.34\n",
      "Unproblematic behavior of distilgpt2: 0.18\n",
      "\n",
      "Category 'coordinate-other-ais':\n",
      "- Disagreement rate: 0.24\n",
      "- Scores: gpt2 0.34, distilgpt2 0.18\n"
     ]
    }
   ],
   "source": [
    "# For this dataset we want to see how disagreement varies for categories\n",
    "scores = experiment.get_feature('LogDisagreement', models=(model.name, model2.name)).values\n",
    "disagreement_rate = sum(scores>0)/len(scores)\n",
    "print(f\"Overall disagreement rate: {disagreement_rate:.2f}\")\n",
    "\n",
    "for m in [model, model2]:\n",
    "    accuracy = np.mean(experiment.get_feature('PredictionCorrectness', model=m.name).values)\n",
    "    print(f\"Unproblematic behavior of {m.name}: {accuracy:.2f}\")\n",
    "\n",
    "categories = experiment.get_feature('Category').values\n",
    "for category in sorted(list(set(categories))):\n",
    "    category_mask = categories==category\n",
    "    category_selection = experiment.sample(mask=category_mask)\n",
    "    \n",
    "    category_disagreement = np.array(category_selection.get_feature('LogDisagreement', models=(model.name, model2.name)).values)\n",
    "    accuracy1 = np.mean(category_selection.get_feature('PredictionCorrectness', model=model.name).values)\n",
    "    accuracy2 = np.mean(category_selection.get_feature('PredictionCorrectness', model=model2.name).values)\n",
    "    print(f\"\\nCategory '{category}':\")\n",
    "    print(f\"- Disagreement rate: {sum(category_disagreement>0)/len(category_disagreement):.2f}\")\n",
    "    print(f\"- Scores: {model.name} {accuracy1:.2f}, {model2.name} {accuracy2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb75ce-ca4f-4629-8a07-7675fd7c92b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
